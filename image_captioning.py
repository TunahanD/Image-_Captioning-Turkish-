# -*- coding: utf-8 -*-
"""Image Captioning(Kendim).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lTpgcSUTI4H0Yxy6i4C4AoyLI0UPaf9p

## DRIVE BAĞLANTISI YAPILMASI
"""

from google.colab import drive
drive.mount("/content/gdrive")

"""## KÜTÜPHANLERİN KURULMASI"""

import os # İşletim sistemi ortaklığı sağlar.

os.environ["KERAS_BACKEND"] = "tensorflow" # Keras kütüphanesi tensorflow'u backend olarak kullanmakta.

import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
import keras
from keras import layers
from keras.applications import efficientnet
from keras.layers import TextVectorization

keras.utils.set_random_seed(111)

"""## VERİ SETİNİN İNDİRİLMESİ"""

!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
!unzip -qq Flickr8k_Dataset.zip
!rm Flickr8k_Dataset.zip

"""## DEĞİŞKENLERİN TANIMLANMASI"""

IMAGES_PATH = "Flicker8k_Dataset" # Veri setinin yolunu verdim.


IMAGE_SIZE = (299, 299) # Kullanılacak görüntüler yeniden boyutlandırıldı.


VOCAB_SIZE = 10000 # En sık kullanılan 10000 kelime,kelime haznesine dahil edilir.


SEQ_LENGTH = 30 # Bir dizinin maksimum uzunluğunu belirtir. Bu uzunluk kelime sayısını ifade eder.   ÖNEMLİ! 55 yetmiyor galiba! Cümle UNK olarak yazıyor.


EMBED_DIM = 512 # Her bir görüntüyü kaç boyutlu vektöre ayırdığını ifade eder.


FF_DIM = 512 # Öğrenme kapasitesini belirler ve genelde Embed_DIM ile aynı boyutta olur.


BATCH_SIZE = 64 # Her adımda kullanılacak örnek sayısı belirlenir.
EPOCHS = 30
AUTOTUNE = tf.data.AUTOTUNE # Veri ardışık düzenini otomatik olarak ayarlayarak veri işleme verimliliği arttırılır.

"""## VERİ SETİNİN HAZIRLANMASI"""

def load_captions_data(filename): # belirtilen dosyadaki başlık (caption) verilerini yükler ve bunları ilgili resimlerle eşleştirir.
    """Loads captions (text) data and maps them to corresponding images.

    Args:
        filename: Path to the text file containing caption data.

    Returns:
        caption_mapping: Dictionary mapping image names and the corresponding captions
        text_data: List containing all the available captions
    """

    with open(filename) as caption_file:
        caption_data = caption_file.readlines()
        caption_mapping = {}
        text_data = []
        images_to_skip = set()

        for line in caption_data:
            line = line.rstrip("\n")
            img_name, caption = line.split("\t")  # Resim adı ve açıklamalar ayrılır.

            # Her görüntü 5 kez tekrar edilir.
            # Bunun nedeni her birinin farklı bir açıklamaya sahip olmasıdır.
            img_name = img_name.split("#")[0]
            img_name = os.path.join(IMAGES_PATH, img_name.strip())

            # Çok uzun ya da çok kısa olan altyazılar kaldırılır.
            tokens = caption.strip().split() # caption.strip metodu cümlenin başındaki ve sonundaki boşlukları kaldırır.
                                             # split metodu ile de boşluklara göre kelimeler ayrılır.

            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:
                images_to_skip.add(img_name)
                continue

            if img_name.endswith("jpg") and img_name not in images_to_skip:
                caption = "<start> " + caption.strip() + " <end>"  # Her başlığa başlangıç ve bitiş belirteci eklenir.
                text_data.append(caption)

                if img_name in caption_mapping:
                    caption_mapping[img_name].append(caption)
                else:
                    caption_mapping[img_name] = [caption]

        for img_name in images_to_skip:
            if img_name in caption_mapping:
                del caption_mapping[img_name]

        return caption_mapping, text_data


def train_val_split(caption_data, train_size=0.8, shuffle=True): # Eğitim ve doğrulama veri kümeleri oluşturulur.
    """Split the captioning dataset into train and validation sets.

    Args:
        caption_data (dict): Dictionary containing the mapped caption data
        train_size (float): Fraction of all the full dataset to use as training data
        shuffle (bool): Whether to shuffle the dataset before splitting

    Returns:
        Traning and validation datasets as two separated dicts
    """

     # 1. Tüm resim isimleri listeye alındı.
    all_images = list(caption_data.keys())

    # 2. Gerekirse karıştırılır.
    if shuffle:
        np.random.shuffle(all_images)

   # 3. Eğitim ve doğrulama setlerine bölünür.
    train_size = int(len(caption_data) * train_size)

    training_data = {
        img_name: caption_data[img_name] for img_name in all_images[:train_size]
    }
    validation_data = {
        img_name: caption_data[img_name] for img_name in all_images[train_size:]
    }

    # 4. Return the splits
    return training_data, validation_data



captions_mapping, text_data = load_captions_data("/content/gdrive/MyDrive/Staj Proje (Kendim)/aciklamalar.txt") # Tokenlarına ayrılmış veri seti yüklenir.

# Veri kümesi eğitim ve doğrulama kümelerine ayrılır.
train_data, valid_data = train_val_split(captions_mapping)
print("Eğitim Örneklerinin Sayısı:",len(train_data))
print("Doğrulama Örneklerinin Sayısı:",len(valid_data))

"""## METİN VERİLERİNİ VEKTÖRLEŞTİRME"""

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string) # Metni küçük harflere çevirir.
    return tf.strings.regex_replace(lowercase, "[%s]" % re.escape(strip_chars), "")


strip_chars = "!\"#$%&'()*+,-./:;<=>?@[\]^_`{|}~" # Özel karakterler çıkartılır.
strip_chars = strip_chars.replace("<", "")
strip_chars = strip_chars.replace(">", "")

vectorization = TextVectorization(
    max_tokens=VOCAB_SIZE, # Kelime haznesinin max boyutunu belirler. Tokenlarına ayrılır.
    output_mode="int",
    output_sequence_length=SEQ_LENGTH,
    standardize=custom_standardization,
)
vectorization.adapt(text_data)

image_augmentation = keras.Sequential( # Görüntü verilerinin arttırılması.
    [
        layers.RandomFlip("horizontal"), # Tensorflow Keras API'sinde bulunan rastgele yatay olarak çeviren katmandır.
        layers.RandomRotation(0.2), # Görüntüleri rastgele 0,2 radyan döndürür.
        layers.RandomContrast(0.3), # Görüntülerin kontrastını %30 oranında değiştirir.
    ]
)

"""## EĞİTİM İÇİN İŞLEM HATTI OLUŞTURMA"""

# Verilen metin dosyasından açıklamaları yükleme ve işleme
def load_and_process_captions(filename, max_length=35, num_captions=5):
    with open(filename) as f:
        lines = f.readlines()

    captions_dict = {}
    for line in lines:
        line = line.rstrip("\n")
        img_name, caption = line.split("\t")
        img_name = img_name.split("#")[0]
        img_name = os.path.join(IMAGES_PATH, img_name.strip())
        caption = "<start> " + caption.strip() + " <end>"

        # Açıklamayı belirli bir maksimum uzunluğa kadar kes
        tokens = caption.split()
        if len(tokens) > max_length:
            caption = ' '.join(tokens[:max_length])

        if img_name in captions_dict:
            captions_dict[img_name].append(caption)
        else:
            captions_dict[img_name] = [caption]

    # Her resim için belirli sayıda açıklama sağla
    for img_name in captions_dict:
        if len(captions_dict[img_name]) < num_captions:
            diff = num_captions - len(captions_dict[img_name])
            captions_dict[img_name].extend(["<start> <end>"] * diff)
        elif len(captions_dict[img_name]) > num_captions:
            captions_dict[img_name] = captions_dict[img_name][:num_captions]

    return captions_dict

# Dosyayı yükle ve işleme tabi tut
filename = "/content/gdrive/MyDrive/Staj Proje (Kendim)/aciklamalar.txt"
captions_dict = load_and_process_captions(filename)

# Dataset oluşturma
def decode_and_resize(img_path):
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMAGE_SIZE)
    img = tf.image.convert_image_dtype(img, tf.float32)
    return img

def process_input(img_path, captions):
    return decode_and_resize(img_path), vectorization(captions)

def make_dataset(images, captions):
    dataset = tf.data.Dataset.from_tensor_slices((images, captions))
    dataset = dataset.shuffle(BATCH_SIZE * 8)
    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)
    return dataset

images = list(captions_dict.keys())
captions = list(captions_dict.values())

# Eğitim ve Doğrulama dataset oluşturma
train_size = int(0.8 * len(images))
train_images = images[:train_size]
train_captions = captions[:train_size]

val_images = images[train_size:]
val_captions = captions[train_size:]

train_dataset = make_dataset(train_images, train_captions)
valid_dataset = make_dataset(val_images, val_captions)

"""## MODELİ OLUŞTURMA"""

def get_cnn_model():
    base_model = efficientnet.EfficientNetB0(
        input_shape=(*IMAGE_SIZE, 3),
        include_top=False,
        weights="imagenet",
    )
    # Özellik çıkarıcı durdurulur. Bu sayede her eğitimde farklı özellikler bulunmaz ve model daha verimli çalışır.
    base_model.trainable = False
    base_model_out = base_model.output
    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)
    cnn_model = keras.models.Model(base_model.input, base_model_out)
    return cnn_model


class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.0
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.dense_1 = layers.Dense(embed_dim, activation="relu")

    def call(self, inputs, training, mask=None):
        inputs = self.layernorm_1(inputs)
        inputs = self.dense_1(inputs)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=None,
            training=training,
        )
        out_1 = self.layernorm_2(inputs + attention_output_1)
        return out_1


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_tokens = embedded_tokens * self.embed_scale
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)


class TransformerDecoderBlock(layers.Layer):
    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.ff_dim = ff_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1
        )
        self.ffn_layer_1 = layers.Dense(ff_dim, activation="relu")
        self.ffn_layer_2 = layers.Dense(embed_dim)

        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()

        self.embedding = PositionalEmbedding(
            embed_dim=EMBED_DIM,
            sequence_length=SEQ_LENGTH,
            vocab_size=VOCAB_SIZE,
        )
        self.out = layers.Dense(VOCAB_SIZE, activation="softmax")

        self.dropout_1 = layers.Dropout(0.3)
        self.dropout_2 = layers.Dropout(0.5)
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, training, mask=None):
        inputs = self.embedding(inputs)
        causal_mask = self.get_causal_attention_mask(inputs)

        if mask is not None:
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=combined_mask,
            training=training,
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
            training=training,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [
                tf.expand_dims(batch_size, -1),
                tf.constant([1, 1], dtype=tf.int32),
            ],
            axis=0,
        )
        return tf.tile(mask, mult)


class ImageCaptioningModel(keras.Model):
    def __init__(
        self,
        cnn_model,
        encoder,
        decoder,
        num_captions_per_image=5,
        image_aug=None,
    ):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.acc_tracker = keras.metrics.Mean(name="accuracy")
        self.num_captions_per_image = num_captions_per_image
        self.image_aug = image_aug

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)

    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)

    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):
        encoder_out = self.encoder(img_embed, training=training)
        batch_seq_inp = batch_seq[:, :-1]
        batch_seq_true = batch_seq[:, 1:]
        mask = tf.math.not_equal(batch_seq_true, 0)
        batch_seq_pred = self.decoder(
            batch_seq_inp, encoder_out, training=training, mask=mask
        )
        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)
        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)
        return loss, acc

    def train_step(self, batch_data): # Eğitim adımı
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        if self.image_aug:
            batch_img = self.image_aug(batch_img)

        # 1. Resim yerleştirmeleri alınır.
        img_embed = self.cnn_model(batch_img)

        # 2. Beş başlığın her birini kod çözücüye birer birer iletilir.
        # Kodlayıcı çıkışlarıyla birlikte kaybı ve doğruluğu hesaplanır. Bunlar her başlık için yapılır.
        for i in range(self.num_captions_per_image):
            with tf.GradientTape() as tape:
                loss, acc = self._compute_caption_loss_and_acc(
                    img_embed, batch_seq[:, i, :], training=True
                )

                # 3 Kayıp ve doğruluk güncellemesi yapılır.
                batch_loss += loss
                batch_acc += acc

            # 4. Eğitilebilir tüm ağırlıkların listesini alınır.
            train_vars = (
                self.encoder.trainable_variables + self.decoder.trainable_variables
            )

            # 5. Dereceler alınır.
            grads = tape.gradient(loss, train_vars)

            # 6. Eğitilebilir ağırlıkları güncellenir.
            self.optimizer.apply_gradients(zip(grads, train_vars))

        # 7. Update the trackers
        batch_acc /= float(self.num_captions_per_image)
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 8. Kayıp ve doğruluk değerleri döndürülür.
        return {
            "loss": self.loss_tracker.result(),
            "acc": self.acc_tracker.result(),
        }

    def test_step(self, batch_data): # Test aşaması
        batch_img, batch_seq = batch_data
        batch_loss = 0
        batch_acc = 0

        # 1. Get image embeddings
        img_embed = self.cnn_model(batch_img)

        # 2. Pass each of the five captions one by one to the decoder
        # along with the encoder outputs and compute the loss as well as accuracy
        # for each caption.
        for i in range(self.num_captions_per_image):
            loss, acc = self._compute_caption_loss_and_acc(
                img_embed, batch_seq[:, i, :], training=False
            )

            # 3. Update batch loss and batch accuracy
            batch_loss += loss
            batch_acc += acc

        batch_acc /= float(self.num_captions_per_image)

        # 4. Update the trackers
        self.loss_tracker.update_state(batch_loss)
        self.acc_tracker.update_state(batch_acc)

        # 5. Return the loss and accuracy values
        return {
            "loss": self.loss_tracker.result(),
            "acc": self.acc_tracker.result(),
        }

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker, self.acc_tracker]


cnn_model = get_cnn_model()
encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)
decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model,
    encoder=encoder,
    decoder=decoder,
    image_aug=image_augmentation,
)

"""## MODEL EĞİTİMİ"""

# Kayıp Fonksiyonu
cross_entropy = keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    reduction='auto', # hata verirse none yazabilirsin.
)

# Eğerki aşırı öğrenme olursa erkenden durur.
early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)


# Öğrenme Oranı (Öğrenme oranı çok yüksekse, model optimum çözümü kaçırabilir; çok düşükse, öğrenme çok yavaş olur.)
class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, post_warmup_learning_rate, warmup_steps):
        super().__init__()
        self.post_warmup_learning_rate = post_warmup_learning_rate
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        global_step = tf.cast(step, tf.float32)
        warmup_steps = tf.cast(self.warmup_steps, tf.float32)
        warmup_progress = global_step / warmup_steps
        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress
        return tf.cond(
            global_step < warmup_steps,
            lambda: warmup_learning_rate,
            lambda: self.post_warmup_learning_rate,
        )



num_train_steps = len(train_dataset) * EPOCHS # batchler hesaplanır.
num_warmup_steps = num_train_steps // 15 # eğitim sürecinin ilk 1/15'lik kısmında kaç adımın warmup (kademeli öğrenme oranı artırma) için kullanılacağını belirlemek amacıyla yapılır.
                                         # Bu adım yapılmazsa dengesiz öğrenme,optimuma ulaşamama veya aşırı öğrenme gibi sorunlarla karşılaşılabilir.
lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps) # Öğrenme oranını ilk aşamada artırıp sonra sabit tutacak bir program oluşturur.

# Model Derleme => Modelin öğrenme sürecini başlatmak için bu adım gereklidir. Optimizasyon yöntemi, kayıp fonksiyonunu minimize etmek için modelin ağırlıklarını nasıl güncelleyeceğini belirler.
caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)

# Model Eğitimi
caption_model.fit(
    train_dataset,
    epochs=EPOCHS,
    validation_data=valid_dataset,
    callbacks=[early_stopping],
)

"""## ÖRNEK METİNLERİ KONTROL ETME"""

vocab = vectorization.get_vocabulary() # Modelin öğrendiği tüm kelimeleri içerir.
index_lookup = dict(zip(range(len(vocab)), vocab)) # Her kelime bir indeks numarası ile eşleştirilir.
max_decoded_sentence_length = SEQ_LENGTH - 1 # Üretilen altyazının maksimum uzunluğunu belirler.
valid_images = list(valid_data.keys()) # Doğrulama veri setindeki tüm görüntülerin listesini oluşturur.

def generate_captions(n=1):
    for _ in range(n):
        sample_img = np.random.choice(valid_images) # Doğrulama veri kümesinden rastgele veriler seçer.


        sample_img = decode_and_resize(sample_img) # Seçilen görüntü bellekteki yerinden okunur, yeniden boyutlandırılır. Model için uygun hale getirilir.
        img = sample_img.numpy().clip(0, 255).astype(np.uint8)
        plt.imshow(img) # Seçilen görüntü matplotlib ile gösterilir.
        plt.show()

        # Görüntü CNN modeline verilir.
        img = tf.expand_dims(sample_img, 0)
        img = caption_model.cnn_model(img)

        # Özeliklerin transformer Encoder'e verilmesi
        encoded_img = caption_model.encoder(img, training=False)

        # Altyazının transformer Decoder ile üretilmesi
        decoded_caption = "<start> "
        for i in range(max_decoded_sentence_length):
            tokenized_caption = vectorization([decoded_caption])[:, :-1]
            mask = tf.math.not_equal(tokenized_caption, 0)
            predictions = caption_model.decoder(
                tokenized_caption, encoded_img, training=False, mask=mask
            )
            sampled_token_index = np.argmax(predictions[0, i, :])
            sampled_token = index_lookup[sampled_token_index]
            if (sampled_token == "<end>") or (sampled_token_index == 0):
                break
            decoded_caption += " " + sampled_token

        decoded_caption = decoded_caption.replace("<start> ", "")
        decoded_caption = decoded_caption.replace(" <end>", "").strip()
        print("Tahmin Edilen Altyazı : ", decoded_caption) # Etiketlerden arınan altyazı gösterilir.

# Belirtilen n sayısı kadar tahmin edilir.
generate_captions(n=3)

from PIL import Image
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

vocab = vectorization.get_vocabulary() # Modelin öğrendiği tüm kelimeler
index_lookup = dict(zip(range(len(vocab)), vocab)) # Kelimelere indeks atama
max_decoded_sentence_length = SEQ_LENGTH - 1

def load_and_preprocess_image(image_path):
    img = Image.open(image_path)
    img = img.resize((299, 299))  # Gerekli boyutlandırmayı modeline göre değiştir
    img = np.array(img) / 255.0  # Normalize et
    img = np.expand_dims(img, 0)  # Modelin kabul edeceği şekle getir
    return img

def generate_caption_for_custom_image(image_path):
    sample_img = load_and_preprocess_image(image_path)
    plt.imshow(np.squeeze(sample_img))
    plt.show()

    # Görüntü CNN modeline verilir.
    img = caption_model.cnn_model(sample_img)

    # Özelliklerin transformer Encoder'a verilmesi
    encoded_img = caption_model.encoder(img, training=False)

    # Altyazının transformer Decoder ile üretilmesi
    decoded_caption = "<start> "
    for i in range(max_decoded_sentence_length):
        tokenized_caption = vectorization([decoded_caption])[:, :-1]
        mask = tf.math.not_equal(tokenized_caption, 0)
        predictions = caption_model.decoder(
            tokenized_caption, encoded_img, training=False, mask=mask
        )
        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = index_lookup[sampled_token_index]
        if (sampled_token == "<end>") or (sampled_token_index == 0):
            break
        decoded_caption += " " + sampled_token

    decoded_caption = decoded_caption.replace("<start> ", "")
    decoded_caption = decoded_caption.replace(" <end>", "").strip()
    print("Tahmin Edilen Altyazı: ", decoded_caption)

# Örnek kullanım
image_path = "/content/gdrive/MyDrive/Staj Proje (Kendim)/depositphotos_174175258-stock-photo-man-talking-mobile-phone-cellphone.jpg"  # Buraya dışarıdan gelecek görüntünün yolunu ekle
generate_caption_for_custom_image(image_path)